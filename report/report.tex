% TODO: Pictures

\documentclass[12pt, a4paper]{article}

\usepackage{preamble}

% TODO: Decide who focuses on which parts. There will of course be overlap and
% we will all review and discuss the different sections.

\title{Models of Associative Memory}

\author{Wenting Jin, Lucas Arnstr√∂m \& Robin Eklind}

\begin{document}

\maketitle

\tableofcontents

\clearpage

% TODO: Add abstract?

% === [ Introduction ] =========================================================

\section{Introduction}

\subsection{Associative Memory}

Definition of associative memory.

Distinction between auto-associative and hetero-associative memory.

% === [ Models of Associative Memory ] =========================================

\section{Models of Associative Memory}

\subsection{Hopfield Networks}

% Wenting.

In paper from J.J.Hopfield [1982], the drawbacks of Perceptron were addressed through its intractable back-coupling, lack of abstraction properties and requirement of synchrony. Information storage was improved with help of Nonlinearity, and emergent computational properties were obtained from simple properties of many cells rather than complex circuitry(which is a result of linear associativity). The input-output relationship of nonlinear computation and binary threshold units were introduced.

Collective behaviors of the model was studied and resulted with the following findings: a few stable states was resulted from most of the initial state space, properties necessary for a physical content-addressable memory were not dependent on the symmetry of the connectivity matrix $T_{ij}$. Statement supported by findings from experiments is that "about 0.15N(memory storage bits) states can be simultaneously remembered before error in recall is severe". Case with arbitrary starting state was studied and results of memories near to the starting state was highly produced. The nominally assigned memories which were called "attractors" dominates the phase flow whereas the flow is not entirely deterministic, which leads to a convergence to local optimum.

Case of consistent internal correlations in the memory was as well adressed, and Hebb synaspses was used and slightly modified to generate nonsymmetric term~$\delta T_{ij}$, which limitation of sequence of four states was addressed.

Mostly auto-associative. Also hetero-associative?

\subsection{Bidirectional Associative Memory}

Mostly hetero-associative. Also auto-associative.

\subsection{Boltzmann Machine}

%TODO The language in this section is horrible!!!

The "Boltzmann Machine" (BM) is a form of "parallel constraint satisfaction network" \cite{ackley1985learning}. It is capable of learning the underlying constraints of a domain by only being shown examples of it. It is very similar to the Hopfield network as it also defines a global energy of the network. The equation that defines the global energy of the machine is identical to that of the Hopfield network.

The difference between the two networks are that the nodes of a Boltzmann Machine are stochastic in their nature.

The BM can be used for constraint satisfaction problems that involve a large amount of weak constraints.

The BM is composed of binary nodes, that is, the nodes are either in an off state or an on state. The nodes get their states determined by a probabilistic function based on the states of their neighbors; where a neighbor is an other node connected by bidirectional connection with a symmetric weight. That is, the connection has a weight that is identical in either direction.

%\subsection{Hamming networks}

%foo

% TODO: Decide whether to include Hamming networks or not.

\subsection{Memory Resistor}

%Robin.

foo

\cite{memsistor} \cite{ahah}

% === [ Current Capabilities ] =================================================

\section{Current Capabilities}

Definitions of capacity.

\begin{itemize}
\item Absolute capacity.
\item Relative capacity.
\item Capacity of associative memory
\item Relative capacity of recalling process
\end{itemize}

\subsection{Hopfield Network}

%Wenting.

1982, $0.15n$ (capacity of associative memory)

% Above 0.15n releases the constraint on symmetries according to Olle.

1985, proven $ \frac{n}{2\log{n}} $ (absolute capacity)

1985, $0.14n$ (relative capacity of recalling process)

1993, $ n ~= 0.4n $ (new result, absolute capacity)

\subsection{Bidirection Associative Memory}

% TODO: Check.

foo

\subsection{Boltzmann Machine}

%Lucas.

Pure Boltzman does not scale, thus impractical.

Restricted Boltzman, basis for deep learning today, using multiple layers.

\subsection{Memory Resistor}

%Robin.

Proof of concept in 2010, using 3 neurons and 2 synapses to achieve 1 associative memory formation.

% === [ Future Potential ] =====================================================

\section{Future Potential}

\subsection{Intelligent Systems}

Intelligence defined by ability to make predictions, not behaviour \cite{intelligence_is_prediction}.

\subsection{Energy Efficiency}

Network in its true sense, \textit{the net is doing all the work} \cite{net_doing_all_the_work}.

%Current computer architectures are designed around major bottlenecks, huge amounts of data has to be shuffled back and forth to perform computations. Reaching its limits; transistors now so small that they only allow a single electron to pass through (similar in size to the ion channels). At this scale, problems arise when transistors may allow electrons to pass through, when they shouldn't and wise versa; which leads to unpredictable behaviour (small bursts of ones when should be be all zero, and wise versa.)

%The brain, parallel, error correcting, memory efficient. (not doing a lot of unnecessary data shuffling?)

% === [ Conclusion ] ===========================================================

\section{Conclusion}

foo

% === [ Research Literature ] ==================================================

Preliminary list of references, cited to force inclusion within the bibliography.

% TODO: Add to this list throughout the project.

% Wenting
\cite{computational_abilities} \cite{optical_processing} \cite{capacity_of_nonmonotonic_model} \cite{stimulus_unitization_and_aging}

% Lucas
\cite{ackley1985learning} \cite{capacity_of_hopfield} \cite{high-order_neural_networks} \cite{neural_network_models_for_associative_memory} \cite{sparsely_encoded_associative_memory}

% Robin
\cite{memsistor} \cite{principle_of_neural_associative_memory} \cite{parallel_models_of_associative_memory} \cite{associative_memory_using_small-world_architecture} \cite{associatron} \cite{associative_search_network} \cite{deep_machine_learning} \cite{on_associative_memory} \cite{from_cell_to_cortex} \cite{ahah}

\bibliography{references}

\end{document}
