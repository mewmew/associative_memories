\subsection{Energy Efficiency}

Network in its true sense, \textit{the net is doing all the work} \cite{net_doing_all_the_work}.

\cite{ahah}

%Current computer architectures are designed around major bottlenecks, huge amounts of data has to be shuffled back and forth to perform computations. Reaching its limits; transistors now so small that they only allow a single electron to pass through (similar in size to the ion channels). At this scale, problems arise when transistors may allow electrons to pass through, when they shouldn't and wise versa; which leads to unpredictable behaviour (small bursts of ones when should be be all zero, and wise versa.)

%The brain, parallel, error correcting, memory efficient. (not doing a lot of unnecessary data shuffling?)

% ref: From BrainScales to Human Brain Project Neuromorphic Computing Coming of Age
%
% Human Brain Project.
%
% 50 000 000 synapses, and about 200 000 neurons
%
% 10 000 faster than biological real-time.
%
% Performance.
%
% 10 femtojoul
%
% 1 joul (in super computers) 10^14
%
% Physical models (neuromorphic)
%
% 10 000 000 times more energy efficient than the state-of-the-art HPC (comparable model)
%
% 10 000 less energy efficient than biology.
%
% Including all the overheads.
%
% # Time Scales
%
% Causality detection   | 10^{-4} sec | 0.1 sec          | 10^{-8} sec |
% Synaptic plasticity   | 1 sec       | 1000 sec         | 10^{-4} sec |
% Learning              | 1 day       | 1000 day         | 10 sec      |
% Development           | 1 year      | 1000 year        | 3000 sec    |
%
% 12 orders of magnitude
%
% Evolution             | > millenia  | > 1000 millenia  | > months    |
%
% > 15 orders of magnitude

%# Applications
%
% Reverse engineer biological data

% ref: Schmuker, Michael, Thomas Pfeil, and Martin Paul Nawrot. "A neuromorphic network for generic multivariate data classification."
%
% TODO: Check :) HBP Roadmap for 2023
%
% Spikey (Heidelberg Lab), commercially available.
% 384 neurons
% 100 000 plastic synapses
% 10.000 - 100.000 real-time
% put and pray into Laptop with USB.
%
% Neuromorphic computing
% Consistent concept for non-von Neumann, non-Turing computer architecture.
%
% Watch the market for Resistor memory, once it is there they will use it for HBP, but using CMOS for now, similar to how von-Neumann used Vaccum tubes before the arrival of the transistor.

% ref: http://knowm.org/how-to-build-the-ex-machina-wetware/
%
% Once you understand what is going on, you can start to understand how to harness it. Think about it. If a bunch of particles will spontaneously organize themselves out of a colloidal suspension to dissipate energy then what happens when we control the energy? What happens if we make the maximal-energy-dissipation solution the solution to our problem? Will the particles self-organize to solve our problem? The answer, it turns out, is a resounding “yes”.
%
% Find a way to use nature as nature itself does. To not compute a brain, but to actually build one. To find a way for matter to self-organize on a chip to solve computational problems.
%
% Memory and processing merge, voltages and clock-rates drop and power efficiencies explode.
%
% AHaH Computing is about understanding how to build circuits that adapt or learn to solve your problems and, as a result, dissipate more energy ‘as a reward’. The path to maximal energy dissipation is the path that solves your problem, and the result is that Nature self-organizes to solve your problem.

% ref: http://knowm.org/the-gordon-panthana-dialog/
%
% It of course does not mean you can magically make a circuit that consumes zero energy. It means that calculation of very large numbers of interacting adaptive variables via the separation of memory and processing is overwhelming less efficient than building a very large interacting adaptive system directly. The example was meant to show just how significant this contribution can be, and why having an intrinsically adaptive element (the memristor) is exciting and useful.
%
% Specifically we use the physics of memristors to eliminate the power and time normally associated with synaptic integration and adaptation.
%
% AHaH Computing embraces hardware as part of the solution and designs from the bottom-up (hardware) and top-down (AHaH compatible algorithms). The result is a broadly commercially viable machine learning system that can scale to biological levels of efficiency.

% ref: http://knowm.org/report-from-the-navy-karles-invitational-on-neuro-electronics/
%
% Dr. Kwabena Boahen
%
% Comparing the scaling problem of CMOS transistors to traffic. He showed that as CMOS transistors are scaled down, and the number of electron ‘lanes’ get close to one, the devices exhibit extremely high magnitude noise due to individual electrons becoming trapped in local energy minima (pot holes) and blocking the traffic. In essence, when the transistors are large, any “stuck electrons” are easily bypassed and their contribution to the total current is minimal. However, when the devices approach the natural ‘lane width’ of an electron (about 2.7 nm if a remember correctly), a stuck electron could reduce the current by 50% or even shut down the whole device. The answer to these problems, Dr. Boahen believes (and I agree), lie in distributed fault-tolerant analog architectures inspired by the brain.
